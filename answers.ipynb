{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Spark imports\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "# Dask imports\n",
    "import dask.bag as db\n",
    "import dask.dataframe as df  # you can use Dask bags or dataframes\n",
    "from csv import reader\n",
    "\n",
    "'''\n",
    "INTRODUCTION\n",
    "\n",
    "The goal of this assignment is to implement a basic analysis of textual \n",
    "data using Apache Spark (http://spark.apache.org) and \n",
    "Dask (https://dask.org). \n",
    "'''\n",
    "\n",
    "'''\n",
    "DATASET\n",
    "\n",
    "We will study a dataset provided by the city of Montreal that contains \n",
    "the list of trees treated against the emerald ash borer \n",
    "(https://en.wikipedia.org/wiki/Emerald_ash_borer). The dataset is \n",
    "described at \n",
    "http://donnees.ville.montreal.qc.ca/dataset/frenes-publics-proteges-injection-agrile-du-frene \n",
    "(use Google translate to translate from French to English). \n",
    "\n",
    "We will use the 2015 and 2016 data sets available in directory `data`.\n",
    "'''\n",
    "\n",
    "'''\n",
    "HELPER FUNCTIONS\n",
    "\n",
    "These functions are here to help you. Instructions will tell you when\n",
    "you should use them. Don't modify them!\n",
    "'''\n",
    "\n",
    "#Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "#Useful functions to print RDDs and Dataframes.\n",
    "def toCSVLineRDD(rdd):\n",
    "    '''\n",
    "    This function convert an RDD or a DataFrame into a CSV string\n",
    "    '''\n",
    "    a = rdd.map(lambda row: \",\".join([str(elt) for elt in row]))\\\n",
    "           .reduce(lambda x,y: os.linesep.join([x,y]))\n",
    "    return a + os.linesep\n",
    "\n",
    "def toCSVLine(data):\n",
    "    '''\n",
    "    Convert an RDD or a DataFrame into a CSV string\n",
    "    '''\n",
    "    if isinstance(data, RDD):\n",
    "        return toCSVLineRDD(data)\n",
    "    elif isinstance(data, DataFrame):\n",
    "        return toCSVLineRDD(data.rdd)\n",
    "    return None\n",
    "\n",
    "'''\n",
    "Plain PYTHON implementation\n",
    "\n",
    "To get started smoothly and become familiar with the assignment's \n",
    "technical context (Git, GitHub, pytest, Travis), we will implement a \n",
    "few steps in plain Python.\n",
    "'''\n",
    "\n",
    "#Python answer functions\n",
    "def count(filename):\n",
    "    '''\n",
    "    Write a Python (not DataFrame, nor RDD) script that prints the number of trees (non-header lines) in\n",
    "    the data file passed as first argument.\n",
    "    Test file: tests/test_count.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    with open(filename, newline=\"\", encoding='UTF-8') as csvfile:\n",
    "        lines = csv.reader(csvfile, delimiter=\",\")\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def parks(filename):\n",
    "    '''\n",
    "    Write a Python (not DataFrame, nor RDD) script that prints the number of trees that are *located in a park*.\n",
    "    To get the park location information, have a look at the *Nom_parc* column (name of park).\n",
    "    Test file: tests/test_parks.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks(filename):\n",
    "    '''\n",
    "    Write a Python (not DataFrame, nor RDD) script that prints the list of unique parks where trees\n",
    "    were treated. The list must be ordered alphabetically. Every element in the list must be printed on\n",
    "    a new line.\n",
    "    Test file: tests/test_uniq_parks.py\n",
    "    Note: The return value should be a string with one park name per line\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_counts(filename):\n",
    "    '''\n",
    "    Write a Python (not DataFrame, nor RDD) script that counts the number of trees treated in each park\n",
    "    and prints a list of \"park,count\" pairs in a CSV manner ordered\n",
    "    alphabetically by the park name. Every element in the list must be printed\n",
    "    on a new line.\n",
    "    Test file: tests/test_uniq_parks_counts.py\n",
    "    Note: The return value should be a CSV string\n",
    "          Have a look at the file *tests/list_parks_count.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def frequent_parks_count(filename):\n",
    "    '''\n",
    "    Write a Python (not DataFrame, nor RDD) script that prints the list of the 10 parks with the\n",
    "    highest number of treated trees. Parks must be ordered by decreasing\n",
    "    number of treated trees and by alphabetical order when they have similar number.\n",
    "    Every list element must be printed on a new line.\n",
    "    Test file: tests/test_frequent_parks_count.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/frequent.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def intersection(filename1, filename2):\n",
    "    '''\n",
    "    Write a Python (not DataFrame, nor RDD) script that prints the alphabetically sorted list of\n",
    "    parks that had trees treated both in 2016 and 2015. Every list element\n",
    "    must be printed on a new line.\n",
    "    Test file: tests/test_intersection.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/intersection.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "'''\n",
    "SPARK RDD IMPLEMENTATION\n",
    "\n",
    "You will now have to re-implement all the functions above using Apache \n",
    "Spark's Resilient Distributed Datasets API (RDD, see documentation at \n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html). \n",
    "Outputs must be identical to the ones obtained above in plain Python. \n",
    "However, all operations must be re-implemented using the RDD API, you \n",
    "are not allowed to simply convert results obtained with plain Python to \n",
    "RDDs (this will be checked). Note that the function *toCSVLine* in the \n",
    "HELPER section at the top of this file converts RDDs into CSV strings.\n",
    "'''\n",
    "\n",
    "# RDD functions\n",
    "\n",
    "def count_rdd(filename):\n",
    "    '''\n",
    "    Write a Python script using RDDs that prints the number of trees\n",
    "    (non-header lines) in the data file passed as first argument.\n",
    "    Test file: tests/test_count_rdd.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def parks_rdd(filename):\n",
    "    '''\n",
    "    Write a Python script using RDDs that prints the number of trees that are *located in a park*.\n",
    "    To get the park location information, have a look at the *Nom_parc* column (name of park).\n",
    "    Test file: tests/test_parks_rdd.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_rdd(filename):\n",
    "    '''\n",
    "    Write a Python script using RDDs that prints the list of unique parks where\n",
    "    trees were treated. The list must be ordered alphabetically. Every element\n",
    "    in the list must be printed on a new line.\n",
    "    Test file: tests/test_uniq_parks_rdd.py\n",
    "    Note: The return value should be a CSV string\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_counts_rdd(filename):\n",
    "    '''\n",
    "    Write a Python script using RDDs that counts the number of trees treated in\n",
    "    each park and prints a list of \"park,count\" pairs in a CSV manner ordered\n",
    "    alphabetically by the park name. Every element in the list must be printed\n",
    "    on a new line.\n",
    "    Test file: tests/test_uniq_parks_counts_rdd.py\n",
    "    Note: The return value should be a CSV string\n",
    "          Have a look at the file *tests/list_parks_count.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def frequent_parks_count_rdd(filename):\n",
    "    '''\n",
    "    Write a Python script using RDDs that prints the list of the 10 parks with\n",
    "    the highest number of treated trees. Parks must be ordered by decreasing\n",
    "    number of treated trees and by alphabetical order when they have similar\n",
    "    number.  Every list element must be printed on a new line.\n",
    "    Test file: tests/test_frequent_parks_count_rdd.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/frequent.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def intersection_rdd(filename1, filename2):\n",
    "    '''\n",
    "    Write a Python script using RDDs that prints the alphabetically sorted list\n",
    "    of parks that had trees treated both in 2016 and 2015. Every list element\n",
    "    must be printed on a new line.\n",
    "    Test file: tests/test_intersection_rdd.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/intersection.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "\n",
    "'''\n",
    "SPARK DATAFRAME IMPLEMENTATION\n",
    "\n",
    "You will now re-implement all the tasks above using Apache Spark's \n",
    "DataFrame API (see documentation at \n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html). \n",
    "Outputs must be identical to the ones obtained above in plain Python. \n",
    "Note: all operations must be re-implemented using the DataFrame API, \n",
    "you are not allowed to simply convert results obtained with the RDD API \n",
    "to Data Frames. Note that the function *toCSVLine* in the HELPER \n",
    "section at the top of this file also converts DataFrames into CSV \n",
    "strings.\n",
    "'''\n",
    "\n",
    "# DataFrame functions\n",
    "\n",
    "def count_df(filename):\n",
    "    '''\n",
    "    Write a Python script using DataFrames that prints the number of trees\n",
    "    (non-header lines) in the data file passed as first argument.\n",
    "    Test file: tests/test_count_df.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def parks_df(filename):\n",
    "    '''\n",
    "    Write a Python script using DataFrames that prints the number of trees that are *located in a park*.\n",
    "    To get the park location information, have a look at the *Nom_parc* column (name of park).\n",
    "    Test file: tests/test_parks_df.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_df(filename):\n",
    "    '''\n",
    "    Write a Python script using DataFrames that prints the list of unique parks\n",
    "    where trees were treated. The list must be ordered alphabetically. Every\n",
    "    element in the list must be printed on a new line.\n",
    "    Test file: tests/test_uniq_parks_df.py\n",
    "    Note: The return value should be a CSV string\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_counts_df(filename):\n",
    "    '''\n",
    "    Write a Python script using DataFrames that counts the number of trees\n",
    "    treated in each park and prints a list of \"park,count\" pairs in a CSV\n",
    "    manner ordered alphabetically by the park name. Every element in the list\n",
    "    must be printed on a new line.\n",
    "    Test file: tests/test_uniq_parks_counts_df.py\n",
    "    Note: The return value should be a CSV string\n",
    "          Have a look at the file *tests/list_parks_count.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def frequent_parks_count_df(filename):\n",
    "    '''\n",
    "    Write a Python script using DataFrames that prints the list of the 10 parks\n",
    "    with the highest number of treated trees. Parks must be ordered by\n",
    "    decreasing number of treated trees and by alphabetical order when they have\n",
    "    similar number.  Every list element must be printed on a new line.\n",
    "    Test file: tests/test_frequent_parks_count_df.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/frequent.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def intersection_df(filename1, filename2):\n",
    "    '''\n",
    "    Write a Python script using DataFrames that prints the alphabetically\n",
    "    sorted list of parks that had trees treated both in 2016 and 2015. Every\n",
    "    list element must be printed on a new line.\n",
    "    Test file: tests/test_intersection_df.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/intersection.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    spark = init_spark()\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "'''\n",
    "DASK IMPLEMENTATION (bonus)\n",
    "\n",
    "You will now re-implement all the tasks above using Dask (see \n",
    "documentation at http://docs.dask.org/en/latest). Outputs must be \n",
    "identical to the ones obtained previously. Note: all operations must be \n",
    "re-implemented using Dask, you are not allowed to simply convert \n",
    "results obtained with the other APIs.\n",
    "'''\n",
    "\n",
    "# Dask functions\n",
    "\n",
    "def count_dask(filename):\n",
    "    '''\n",
    "    Write a Python script using Dask that prints the number of trees\n",
    "    (non-header lines) in the data file passed as first argument.\n",
    "    Test file: tests/test_count_dask.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def parks_dask(filename):\n",
    "    '''\n",
    "    Write a Python script using Dask that prints the number of trees that are *located in a park*.\n",
    "    To get the park location information, have a look at the *Nom_parc* column (name of park).\n",
    "    Test file: tests/test_parks_dask.py\n",
    "    Note: The return value should be an integer\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_dask(filename):\n",
    "    '''\n",
    "    Write a Python script using Dask that prints the list of unique parks\n",
    "    where trees were treated. The list must be ordered alphabetically. Every\n",
    "    element in the list must be printed on a new line.\n",
    "    Test file: tests/test_uniq_parks_dask.py\n",
    "    Note: The return value should be a CSV string\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def uniq_parks_counts_dask(filename):\n",
    "    '''\n",
    "    Write a Python script using Dask that counts the number of trees\n",
    "    treated in each park and prints a list of \"park,count\" pairs in a CSV\n",
    "    manner ordered alphabetically by the park name. Every element in the list\n",
    "    must be printed on a new line.\n",
    "    Test file: tests/test_uniq_parks_counts_dask.py\n",
    "    Note: The return value should be a CSV string\n",
    "          Have a look at the file *tests/list_parks_count.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def frequent_parks_count_dask(filename):\n",
    "    '''\n",
    "    Write a Python script using Dask that prints the list of the 10 parks\n",
    "    with the highest number of treated trees. Parks must be ordered by\n",
    "    decreasing number of treated trees and by alphabetical order when they have\n",
    "    similar number.  Every list element must be printed on a new line.\n",
    "    Test file: tests/test_frequent_parks_count_dask.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/frequent.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "def intersection_dask(filename1, filename2):\n",
    "    '''\n",
    "    Write a Python script using Dask that prints the alphabetically\n",
    "    sorted list of parks that had trees treated both in 2016 and 2015. Every\n",
    "    list element must be printed on a new line.\n",
    "    Test file: tests/test_intersection_dask.py\n",
    "    Note: The return value should be a CSV string.\n",
    "          Have a look at the file *tests/intersection.txt* to get the exact return format.\n",
    "    '''\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    raise Exception(\"Not implemented yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.6.12, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: D:\\ALL\\Work\\Coding\\Python\\soen471\\bigdata-la1-Fryingpannn, configfile: pytest.ini\n",
      "collected 0 items / 1 error\n",
      "\n",
      "=================================== ERRORS ====================================\n",
      "____________________ ERROR collecting tests/test_count.py _____________________\n",
      "ImportError while importing test module 'D:\\ALL\\Work\\Coding\\Python\\soen471\\bigdata-la1-Fryingpannn\\tests\\test_count.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "C:\\Users\\mattx\\Anaconda3\\envs\\bigdata-lab\\lib\\importlib\\__init__.py:126: in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "..\\tests\\test_count.py:4: in <module>\n",
      "    from answer import count\n",
      "E   ModuleNotFoundError: No module named 'answer'\n",
      "=========================== short test summary info ===========================\n",
      "ERROR ..\\tests\\test_count.py\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "============================== 1 error in 0.18s ===============================\n"
     ]
    }
   ],
   "source": [
    "!pytest ../tests/test_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.6.12, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: D:\\ALL\\Work\\Coding\\Python\\soen471\\bigdata-la1-Fryingpannn, configfile: pytest.ini\n",
      "collected 1 item\n",
      "\n",
      "tests\\test_count.py F                                                    [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_________________________________ test_count __________________________________\n",
      "\n",
      "    def test_count():\n",
      "        a = count(\"./data/frenepublicinjection2016.csv\")\n",
      ">       assert(a == 27244)\n",
      "E       assert 27246 == 27244\n",
      "\n",
      "tests\\test_count.py:8: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED tests/test_count.py::test_count - assert 27246 == 27244\n",
      "============================== 1 failed in 1.31s ==============================\n"
     ]
    }
   ],
   "source": [
    "!pytest ./tests/test_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-lab]",
   "language": "python",
   "name": "conda-env-bigdata-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
